"""
RAGç³»ç»Ÿå·¥å…·æ¥å£ - åŠ¨æ€åµŒå…¥æ¨¡å‹ç‰ˆ
æ”¯æŒåœ¨è¿è¡Œæ—¶é€‰æ‹©ä½¿ç”¨OllamaåµŒå…¥æˆ–é»˜è®¤åµŒå…¥
"""

import os
from typing import List, Dict, Any, Optional
from langchain_core.tools import tool

current_file = os.path.abspath(__file__)
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(current_file))))  # å‡è®¾åœ¨Toolsç›®å½•ä¸‹

knowledge_base_path = os.path.join(project_root, "Sandbox", "knowledge_base")
vector_store_path = os.path.join(project_root, "Data", "RAG", "vector_store")

# é…ç½®ä¿¡æ¯
RAG_CONFIG = {
    "knowledge_base_path": knowledge_base_path,
    "vector_store_base_path": vector_store_path,
    "auto_load_on_init": False,  # ä¸è‡ªåŠ¨åŠ è½½ï¼ŒæŒ‰éœ€åŠ è½½
    "use_generator": False,
    "supported_formats": [".txt", ".md", ".pdf", ".docx", ".doc", ".html", ".htm"],
    "text_splitter": {
        "chunk_size": 1000,
        "chunk_overlap": 200
    },
    "retrieval": {
        "default_k": 3,
        "similarity_threshold": 0.7
    }
}


def get_vector_store_path(use_ollama_embedding: bool) -> str:
    """
    æ ¹æ®åµŒå…¥æ¨¡å‹ç±»å‹è·å–å¯¹åº”çš„å‘é‡åº“è·¯å¾„

    Args:
        use_ollama_embedding: æ˜¯å¦ä½¿ç”¨OllamaåµŒå…¥æ¨¡å‹

    Returns:
        å‘é‡åº“è·¯å¾„
    """
    base_path = RAG_CONFIG["vector_store_base_path"]
    if use_ollama_embedding:
        return base_path + "/ollama"
    else:
        return base_path + "/default"


def get_rag_system(use_ollama_embedding: bool = False, use_generator: bool = False):
    """
    è·å–RAGç³»ç»Ÿå®ä¾‹

    Args:
        use_ollama_embedding: æ˜¯å¦ä½¿ç”¨OllamaåµŒå…¥æ¨¡å‹
        use_generator: æ˜¯å¦ä½¿ç”¨ç”Ÿæˆå™¨

    Returns:
        RAGç³»ç»Ÿå®ä¾‹
    """
    try:
        from Tools.RAG.main import RAGSystem

        # è·å–å¯¹åº”çš„å‘é‡åº“è·¯å¾„
        vector_store_path = get_vector_store_path(use_ollama_embedding)

        # åˆå§‹åŒ–RAGç³»ç»Ÿ
        rag_system = RAGSystem(
            use_ollama_embedding=use_ollama_embedding,
            use_generator=use_generator,
            persist_directory=vector_store_path
        )

        return rag_system

    except Exception as e:
        raise Exception(f"RAGç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {str(e)}")


def ensure_knowledge_base_loaded(rag_system, use_ollama_embedding: bool):
    """
    ç¡®ä¿çŸ¥è¯†åº“å·²åŠ è½½

    Args:
        rag_system: RAGç³»ç»Ÿå®ä¾‹
        use_ollama_embedding: æ˜¯å¦ä½¿ç”¨OllamaåµŒå…¥æ¨¡å‹
    """
    try:
        knowledge_base_path = RAG_CONFIG["knowledge_base_path"]
        vector_store_path = get_vector_store_path(use_ollama_embedding)

        # æ£€æŸ¥çŸ¥è¯†åº“ç›®å½•æ˜¯å¦å­˜åœ¨
        if not os.path.exists(knowledge_base_path):
            raise Exception(f"çŸ¥è¯†åº“ç›®å½•ä¸å­˜åœ¨: {knowledge_base_path}")

        # æ£€æŸ¥å‘é‡å­˜å‚¨æ˜¯å¦å·²å­˜åœ¨
        if os.path.exists(vector_store_path):
            print(f"ğŸ“š ä½¿ç”¨ç°æœ‰å‘é‡å­˜å‚¨: {vector_store_path}")
            return

        # åŠ è½½çŸ¥è¯†åº“
        print(f"ğŸ“š å¼€å§‹åŠ è½½çŸ¥è¯†åº“: {knowledge_base_path}")
        result = rag_system.ingest_directory(
            directory_path=knowledge_base_path,
            file_extensions=RAG_CONFIG["supported_formats"]
        )

        if result["success"]:
            print(f"âœ… çŸ¥è¯†åº“åŠ è½½å®Œæˆ: å¤„ç†äº† {result['processed_files']} ä¸ªæ–‡ä»¶")
        else:
            raise Exception(f"çŸ¥è¯†åº“åŠ è½½å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

    except Exception as e:
        raise Exception(f"çŸ¥è¯†åº“åŠ è½½å¤±è´¥: {str(e)}")


@tool
def rag_search(question: str, k: int = None, use_ollama_embedding: bool = False) -> List[Dict[str, Any]]:
    """
    ä½¿ç”¨RAGç³»ç»Ÿæ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼ˆæ”¯æŒåŠ¨æ€åµŒå…¥æ¨¡å‹é€‰æ‹©ï¼‰

    Args:
        question: æŸ¥è¯¢é—®é¢˜
        k: è¿”å›æ–‡æ¡£æ•°é‡ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®å€¼
        use_ollama_embedding: æ˜¯å¦ä½¿ç”¨OllamaåµŒå…¥æ¨¡å‹

    Returns:
        æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨ï¼ŒåŒ…å«å†…å®¹å’Œå…ƒæ•°æ®
    """
    try:
        if k is None:
            k = RAG_CONFIG["retrieval"]["default_k"]

        # è·å–RAGç³»ç»Ÿå®ä¾‹
        rag_system = get_rag_system(use_ollama_embedding=use_ollama_embedding, use_generator=False)

        # ç¡®ä¿çŸ¥è¯†åº“å·²åŠ è½½
        ensure_knowledge_base_loaded(rag_system, use_ollama_embedding)

        # æ£€ç´¢æ–‡æ¡£
        documents = rag_system.search(question, k=k)

        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        result = []
        for doc in documents:
            result.append({
                "content": doc.page_content,
                "metadata": doc.metadata,
                "source": doc.metadata.get("source", "unknown")
            })

        return result

    except Exception as e:
        return [{"error": f"æ£€ç´¢å¤±è´¥: {str(e)}"}]


@tool
def rag_query(question: str, k: int = None, use_generator: bool = None, use_ollama_embedding: bool = False) -> Dict[str, Any]:
    """
    ä½¿ç”¨RAGç³»ç»ŸæŸ¥è¯¢çŸ¥è¯†åº“ï¼ˆæ”¯æŒåŠ¨æ€åµŒå…¥æ¨¡å‹é€‰æ‹©ï¼‰

    Args:
        question: æŸ¥è¯¢é—®é¢˜
        k: è¿”å›æ–‡æ¡£æ•°é‡ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®å€¼
        use_generator: æ˜¯å¦ä½¿ç”¨ç”Ÿæˆå™¨ç”Ÿæˆç­”æ¡ˆ
        use_ollama_embedding: æ˜¯å¦ä½¿ç”¨OllamaåµŒå…¥æ¨¡å‹

    Returns:
        æŸ¥è¯¢ç»“æœï¼ŒåŒ…å«æ£€ç´¢åˆ°çš„æ–‡æ¡£å’Œå¯é€‰ç”Ÿæˆçš„ç­”æ¡ˆ
    """
    try:
        if k is None:
            k = RAG_CONFIG["retrieval"]["default_k"]
        if use_generator is None:
            use_generator = RAG_CONFIG["use_generator"]

        # è·å–RAGç³»ç»Ÿå®ä¾‹
        rag_system = get_rag_system(use_ollama_embedding=use_ollama_embedding, use_generator=use_generator)

        # ç¡®ä¿çŸ¥è¯†åº“å·²åŠ è½½
        ensure_knowledge_base_loaded(rag_system, use_ollama_embedding)

        # æŸ¥è¯¢çŸ¥è¯†åº“
        result = rag_system.query(question, k=k, use_generator=use_generator)

        # è½¬æ¢æ–‡æ¡£ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
        if "retrieved_documents" in result:
            docs = []
            for doc in result["retrieved_documents"]:
                docs.append({
                    "content": doc.page_content,
                    "metadata": doc.metadata,
                    "source": doc.metadata.get("source", "unknown")
                })
            result["retrieved_documents"] = docs

        return result

    except Exception as e:
        return {
            "question": question,
            "error": f"æŸ¥è¯¢å¤±è´¥: {str(e)}",
            "retrieved_documents": [],
            "sources_count": 0,
            "used_generator": use_generator if use_generator is not None else RAG_CONFIG["use_generator"]
        }


@tool
def rag_system_info(use_ollama_embedding: bool = False) -> Dict[str, Any]:
    """
    è·å–RAGç³»ç»Ÿä¿¡æ¯ï¼ˆæ”¯æŒåŠ¨æ€åµŒå…¥æ¨¡å‹é€‰æ‹©ï¼‰

    Args:
        use_ollama_embedding: æ˜¯å¦ä½¿ç”¨OllamaåµŒå…¥æ¨¡å‹

    Returns:
        ç³»ç»Ÿé…ç½®å’ŒçŠ¶æ€ä¿¡æ¯
    """
    try:
        # è·å–RAGç³»ç»Ÿå®ä¾‹
        rag_system = get_rag_system(use_ollama_embedding=use_ollama_embedding, use_generator=False)

        system_info = rag_system.get_system_info()

        # æ·»åŠ é…ç½®ä¿¡æ¯
        system_info["config"] = {
            "knowledge_base_path": RAG_CONFIG["knowledge_base_path"],
            "vector_store_path": get_vector_store_path(use_ollama_embedding),
            "auto_load_on_init": RAG_CONFIG["auto_load_on_init"],
            "supported_formats": RAG_CONFIG["supported_formats"]
        }

        return system_info

    except Exception as e:
        return {"error": f"è·å–ç³»ç»Ÿä¿¡æ¯å¤±è´¥: {str(e)}"}


@tool
def rag_refresh(use_ollama_embedding: bool = False) -> Dict[str, Any]:
    """
    åˆ·æ–°çŸ¥è¯†åº“ - é‡æ–°åŠ è½½çŸ¥è¯†åº“æ–‡æ¡£ï¼ˆæ”¯æŒåŠ¨æ€åµŒå…¥æ¨¡å‹é€‰æ‹©ï¼‰

    Args:
        use_ollama_embedding: æ˜¯å¦ä½¿ç”¨OllamaåµŒå…¥æ¨¡å‹

    Returns:
        åˆ·æ–°æ“ä½œç»“æœ
    """
    try:
        knowledge_base_path = RAG_CONFIG["knowledge_base_path"]

        # æ£€æŸ¥çŸ¥è¯†åº“ç›®å½•æ˜¯å¦å­˜åœ¨
        if not os.path.exists(knowledge_base_path):
            return {
                "success": False,
                "error": f"çŸ¥è¯†åº“ç›®å½•ä¸å­˜åœ¨: {knowledge_base_path}"
            }

        print(f"ğŸ”„ å¼€å§‹åˆ·æ–°çŸ¥è¯†åº“: {knowledge_base_path}")

        # è·å–RAGç³»ç»Ÿå®ä¾‹
        rag_system = get_rag_system(use_ollama_embedding=use_ollama_embedding, use_generator=False)

        # é‡æ–°åŠ è½½çŸ¥è¯†åº“
        result = rag_system.ingest_directory(
            directory_path=knowledge_base_path,
            file_extensions=RAG_CONFIG["supported_formats"]
        )

        if result["success"]:
            return {
                "success": True,
                "message": f"çŸ¥è¯†åº“åˆ·æ–°å®Œæˆ: å¤„ç†äº† {result['processed_files']} ä¸ªæ–‡ä»¶",
                "details": result
            }
        else:
            return {
                "success": False,
                "error": result.get("error", "æœªçŸ¥é”™è¯¯"),
                "details": result
            }

    except Exception as e:
        return {
            "success": False,
            "error": f"çŸ¥è¯†åº“åˆ·æ–°å¤±è´¥: {str(e)}"
        }


@tool
def rag_clear_knowledge_base(use_ollama_embedding: bool = False) -> Dict[str, Any]:
    """
    æ¸…ç©ºRAGçŸ¥è¯†åº“ï¼ˆæ”¯æŒåŠ¨æ€åµŒå…¥æ¨¡å‹é€‰æ‹©ï¼‰

    Args:
        use_ollama_embedding: æ˜¯å¦ä½¿ç”¨OllamaåµŒå…¥æ¨¡å‹

    Returns:
        æ¸…ç©ºæ“ä½œç»“æœ
    """
    try:
        # è·å–RAGç³»ç»Ÿå®ä¾‹
        rag_system = get_rag_system(use_ollama_embedding=use_ollama_embedding, use_generator=False)

        success = rag_system.clear_knowledge_base()
        return {
            "success": success,
            "message": "çŸ¥è¯†åº“å·²æ¸…ç©º" if success else "æ¸…ç©ºå¤±è´¥"
        }

    except Exception as e:
        return {
            "success": False,
            "error": f"æ¸…ç©ºçŸ¥è¯†åº“å¤±è´¥: {str(e)}"
        }


# å·¥å…·åˆ—è¡¨
RAG_TOOLS = [
    rag_search,
    rag_query,
    rag_system_info,
    rag_refresh,
    rag_clear_knowledge_base,
]


def get_rag_tools():
    """
    è·å–æ‰€æœ‰åŠ¨æ€RAGå·¥å…·

    Returns:
        RAGå·¥å…·åˆ—è¡¨
    """
    return RAG_TOOLS


# æµ‹è¯•å‡½æ•°
def test_rag_tools():
    """æµ‹è¯•åŠ¨æ€RAGå·¥å…·"""
    print("ğŸ§ª æµ‹è¯•åŠ¨æ€RAGå·¥å…·...")
    # æµ‹è¯•åˆ·æ–°
    try:
        refresh_result = rag_refresh(use_ollama_embedding=False)
        print(f"   åˆ·æ–°æµ‹è¯•: {refresh_result.get('success', False)}")
        refresh_result = rag_refresh(use_ollama_embedding=True)
        print(f"   åˆ·æ–°æµ‹è¯•: {refresh_result.get('success', False)}")
        print("âœ… åˆ·æ–°å·¥å…·æµ‹è¯•é€šè¿‡")
    except Exception as e:
        print(f"   âš ï¸ åˆ·æ–°å·¥å…·æµ‹è¯•å¤±è´¥: {str(e)}")

    # æµ‹è¯•ç³»ç»Ÿä¿¡æ¯
    try:
        info_default = rag_system_info(use_ollama_embedding=False)
        print(f"   é»˜è®¤åµŒå…¥ç³»ç»Ÿä¿¡æ¯: {info_default.get('embedding_model', {})}")

        info_ollama = rag_system_info(use_ollama_embedding=True)
        print(f"   OllamaåµŒå…¥ç³»ç»Ÿä¿¡æ¯: {info_ollama.get('embedding_model', {})}")

        print("âœ… ç³»ç»Ÿä¿¡æ¯å·¥å…·æµ‹è¯•é€šè¿‡")
    except Exception as e:
        print(f"   âš ï¸ ç³»ç»Ÿä¿¡æ¯å·¥å…·æµ‹è¯•å¤±è´¥: {str(e)}")

    # æµ‹è¯•æœç´¢
    try:
        results_default = rag_search("ä»€ä¹ˆæ˜¯Langchain", k=2, use_ollama_embedding=False)
        print(f"   é»˜è®¤åµŒå…¥æœç´¢: æ‰¾åˆ° {len(results_default)} ä¸ªæ–‡æ¡£")

        results_ollama = rag_search("ä»€ä¹ˆæ˜¯Langchain", k=2, use_ollama_embedding=True)
        print(f"   OllamaåµŒå…¥æœç´¢: æ‰¾åˆ° {len(results_ollama)} ä¸ªæ–‡æ¡£")

        print("âœ… æœç´¢å·¥å…·æµ‹è¯•é€šè¿‡")
    except Exception as e:
        print(f"   âš ï¸ æœç´¢å·¥å…·æµ‹è¯•å¤±è´¥: {str(e)}")

    print("ğŸ¯ åŠ¨æ€RAGå·¥å…·æµ‹è¯•å®Œæˆ")


if __name__ == "__main__":
    test_rag_tools()