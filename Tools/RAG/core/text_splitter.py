"""
æ–‡æœ¬åˆ†å‰²å™¨æ¨¡å—
è´Ÿè´£å°†å¤§æ–‡æ¡£åˆ†å‰²ä¸ºé€‚åˆå‘é‡åŒ–çš„å°å—
"""

from typing import List
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter


class TextSplitter:
    """æ–‡æœ¬åˆ†å‰²å™¨ç±»"""
    
    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):
        """
        åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
        
        Args:
            chunk_size: æ¯ä¸ªchunkçš„æœ€å¤§å­—ç¬¦æ•°
            chunk_overlap: chunkä¹‹é—´çš„é‡å å­—ç¬¦æ•°
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
        # ä½¿ç”¨é€’å½’å­—ç¬¦æ–‡æœ¬åˆ†å‰²å™¨
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]
        )
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """
        åˆ†å‰²æ–‡æ¡£åˆ—è¡¨
        
        Args:
            documents: è¦åˆ†å‰²çš„æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            åˆ†å‰²åçš„æ–‡æ¡£åˆ—è¡¨
        """
        if not documents:
            return []
        
        try:
            # ä½¿ç”¨LangChainçš„åˆ†å‰²å™¨
            split_docs = self.splitter.split_documents(documents)
            
            # ç¡®ä¿å…ƒæ•°æ®è¢«æ­£ç¡®ä¼ é€’
            for i, doc in enumerate(split_docs):
                # ä¿ç•™åŸå§‹å…ƒæ•°æ®
                if i < len(documents):
                    doc.metadata.update(documents[0].metadata)
                
                # æ·»åŠ åˆ†å‰²ç›¸å…³çš„å…ƒæ•°æ®
                doc.metadata.update({
                    "chunk_size": len(doc.page_content),
                    "chunk_index": i,
                    "total_chunks": len(split_docs)
                })
            
            return split_docs
            
        except Exception as e:
            raise Exception(f"æ–‡æ¡£åˆ†å‰²å¤±è´¥: {str(e)}")
    
    def split_text(self, text: str, metadata: dict = None) -> List[Document]:
        """
        åˆ†å‰²çº¯æ–‡æœ¬
        
        Args:
            text: è¦åˆ†å‰²çš„æ–‡æœ¬
            metadata: å¯é€‰çš„å…ƒæ•°æ®
            
        Returns:
            åˆ†å‰²åçš„æ–‡æ¡£åˆ—è¡¨
        """
        if not text:
            return []
        
        try:
            # åˆ›å»ºä¸´æ—¶æ–‡æ¡£
            temp_doc = Document(
                page_content=text,
                metadata=metadata or {}
            )
            
            # åˆ†å‰²æ–‡æ¡£
            return self.split_documents([temp_doc])
            
        except Exception as e:
            raise Exception(f"æ–‡æœ¬åˆ†å‰²å¤±è´¥: {str(e)}")
    
    def get_splitter_info(self) -> dict:
        """è·å–åˆ†å‰²å™¨é…ç½®ä¿¡æ¯"""
        return {
            "chunk_size": self.chunk_size,
            "chunk_overlap": self.chunk_overlap,
            "splitter_type": "RecursiveCharacterTextSplitter"
        }


# æµ‹è¯•å‡½æ•°
def test_text_splitter():
    """æµ‹è¯•æ–‡æœ¬åˆ†å‰²å™¨"""
    print("ğŸ§ª æµ‹è¯•æ–‡æœ¬åˆ†å‰²å™¨...")
    
    # åˆ›å»ºæµ‹è¯•åˆ†å‰²å™¨
    splitter = TextSplitter(chunk_size=500, chunk_overlap=50)
    
    # æµ‹è¯•1: çŸ­æ–‡æœ¬åˆ†å‰²
    print("ğŸ“ æµ‹è¯•çŸ­æ–‡æœ¬åˆ†å‰²...")
    short_text = "è¿™æ˜¯ä¸€ä¸ªçŸ­æ–‡æœ¬ï¼Œä¸éœ€è¦åˆ†å‰²ã€‚"
    documents = splitter.split_text(short_text, {"source": "test"})
    
    print(f"   çŸ­æ–‡æœ¬åˆ†å‰²ç»“æœ: {len(documents)} ä¸ªæ–‡æ¡£")
    print(f"   æ–‡æ¡£å†…å®¹: {documents[0].page_content}")
    print(f"   æ–‡æ¡£å…ƒæ•°æ®: {documents[0].metadata}")
    
    # æµ‹è¯•2: é•¿æ–‡æœ¬åˆ†å‰²
    print("\nğŸ“ æµ‹è¯•é•¿æ–‡æœ¬åˆ†å‰²...")
    long_text = """
    è¿™æ˜¯ä¸€ä¸ªé•¿æ–‡æœ¬ï¼Œéœ€è¦è¢«åˆ†å‰²æˆå¤šä¸ªchunkã€‚
    
    ç¬¬ä¸€æ®µå†…å®¹ã€‚ç¬¬ä¸€æ®µå†…å®¹ã€‚ç¬¬ä¸€æ®µå†…å®¹ã€‚ç¬¬ä¸€æ®µå†…å®¹ã€‚ç¬¬ä¸€æ®µå†…å®¹ã€‚
    ç¬¬ä¸€æ®µå†…å®¹ã€‚ç¬¬ä¸€æ®µå†…å®¹ã€‚ç¬¬ä¸€æ®µå†…å®¹ã€‚ç¬¬ä¸€æ®µå†…å®¹ã€‚ç¬¬ä¸€æ®µå†…å®¹ã€‚
    
    ç¬¬äºŒæ®µå†…å®¹ã€‚ç¬¬äºŒæ®µå†…å®¹ã€‚ç¬¬äºŒæ®µå†…å®¹ã€‚ç¬¬äºŒæ®µå†…å®¹ã€‚ç¬¬äºŒæ®µå†…å®¹ã€‚
    ç¬¬äºŒæ®µå†…å®¹ã€‚ç¬¬äºŒæ®µå†…å®¹ã€‚ç¬¬äºŒæ®µå†…å®¹ã€‚ç¬¬äºŒæ®µå†…å®¹ã€‚ç¬¬äºŒæ®µå†…å®¹ã€‚
    
    ç¬¬ä¸‰æ®µå†…å®¹ã€‚ç¬¬ä¸‰æ®µå†…å®¹ã€‚ç¬¬ä¸‰æ®µå†…å®¹ã€‚ç¬¬ä¸‰æ®µå†…å®¹ã€‚ç¬¬ä¸‰æ®µå†…å®¹ã€‚
    ç¬¬ä¸‰æ®µå†…å®¹ã€‚ç¬¬ä¸‰æ®µå†…å®¹ã€‚ç¬¬ä¸‰æ®µå†…å®¹ã€‚ç¬¬ä¸‰æ®µå†…å®¹ã€‚ç¬¬ä¸‰æ®µå†…å®¹ã€‚
    
    ç¬¬å››æ®µå†…å®¹ã€‚ç¬¬å››æ®µå†…å®¹ã€‚ç¬¬å››æ®µå†…å®¹ã€‚ç¬¬å››æ®µå†…å®¹ã€‚ç¬¬å››æ®µå†…å®¹ã€‚
    ç¬¬å››æ®µå†…å®¹ã€‚ç¬¬å››æ®µå†…å®¹ã€‚ç¬¬å››æ®µå†…å®¹ã€‚ç¬¬å››æ®µå†…å®¹ã€‚ç¬¬å››æ®µå†…å®¹ã€‚
    """
    
    documents = splitter.split_text(long_text, {"source": "long_test"})
    
    print(f"   é•¿æ–‡æœ¬åˆ†å‰²ç»“æœ: {len(documents)} ä¸ªæ–‡æ¡£")
    for i, doc in enumerate(documents[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
        print(f"   æ–‡æ¡£{i+1}: {doc.page_content[:100]}...")
        print(f"   å…ƒæ•°æ®: chunk_size={doc.metadata.get('chunk_size')}, "
              f"chunk_index={doc.metadata.get('chunk_index')}")
    
    # æµ‹è¯•3: æ–‡æ¡£åˆ—è¡¨åˆ†å‰²
    print("\nğŸ“ æµ‹è¯•æ–‡æ¡£åˆ—è¡¨åˆ†å‰²...")
    from langchain_core.documents import Document
    
    doc1 = Document(page_content="ç¬¬ä¸€ä¸ªæ–‡æ¡£å†…å®¹", metadata={"doc_id": 1})
    doc2 = Document(page_content="ç¬¬äºŒä¸ªæ–‡æ¡£å†…å®¹", metadata={"doc_id": 2})
    
    documents = splitter.split_documents([doc1, doc2])
    
    print(f"   æ–‡æ¡£åˆ—è¡¨åˆ†å‰²ç»“æœ: {len(documents)} ä¸ªæ–‡æ¡£")
    
    # æµ‹è¯•4: åˆ†å‰²å™¨ä¿¡æ¯
    print("\nğŸ“ æµ‹è¯•åˆ†å‰²å™¨ä¿¡æ¯...")
    info = splitter.get_splitter_info()
    print(f"   åˆ†å‰²å™¨é…ç½®: {info}")
    
    print("\nğŸ¯ æ–‡æœ¬åˆ†å‰²å™¨æµ‹è¯•å®Œæˆ")


if __name__ == "__main__":
    test_text_splitter()