"""
RAGç³»ç»Ÿä¸»é›†æˆæ¨¡å—
æ”¯æŒå¯é€‰ç”Ÿæˆå™¨å’Œç›´æ¥æ£€ç´¢ç»“æœ
"""
import shutil
from typing import List, Dict, Any, Optional, Union
from langchain_core.documents import Document

from Tools.RAG.core.document_loader import DocumentLoader
from Tools.RAG.core.text_splitter import TextSplitter
from Tools.RAG.core.embedding_manager import EmbeddingManager
from Tools.RAG.core.vector_store import VectorStoreManager
from Tools.RAG.core.retriever import Retriever
from Tools.RAG.core.generator import Generator


class RAGSystem:
    """RAGç³»ç»Ÿä¸»ç±»"""

    def __init__(self,
                 use_ollama_embedding: bool = False,
                 use_generator: bool = False,
                 persist_directory: str = "./chroma_db",
                 collection_name: str = "rag_collection"):
        """
        åˆå§‹åŒ–RAGç³»ç»Ÿ

        Args:
            use_ollama_embedding: æ˜¯å¦ä½¿ç”¨OllamaåµŒå…¥
            use_generator: æ˜¯å¦ä½¿ç”¨ç”Ÿæˆå™¨
            persist_directory: å‘é‡å­˜å‚¨æŒä¹…åŒ–ç›®å½•
            collection_name: é›†åˆåç§°
        """
        print("ğŸš€ åˆå§‹åŒ–RAGç³»ç»Ÿ...")

        # æ ¸å¿ƒç»„ä»¶
        self.loader = DocumentLoader()
        self.splitter = TextSplitter()

        # åµŒå…¥ç®¡ç†å™¨
        self.embedding_manager = EmbeddingManager(use_ollama=use_ollama_embedding)

        # å‘é‡å­˜å‚¨
        self.vector_store = VectorStoreManager(
            embedding_function=self.embedding_manager.get_embedding_function(),
            persist_directory=persist_directory,
            collection_name=collection_name
        )

        # æ£€ç´¢å™¨
        self.vector_store.initialize_store()  # ç¡®ä¿å‘é‡å­˜å‚¨å·²åˆå§‹åŒ–
        self.retriever = Retriever(self.vector_store.vector_store)

        # å¯é€‰ç”Ÿæˆå™¨
        self.use_generator = use_generator
        if use_generator:
            self.generator = Generator()
            print("ğŸ¤– ç”Ÿæˆå™¨å·²å¯ç”¨")
        else:
            self.generator = None
            print("ğŸ” ç”Ÿæˆå™¨æœªå¯ç”¨ï¼Œå°†ç›´æ¥è¿”å›æ£€ç´¢ç»“æœ")

        print("âœ… RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")

    def ingest_documents(self,
                        file_path: str,
                        chunk_size: int = 1000,
                        chunk_overlap: int = 200) -> Dict[str, Any]:
        """
        æ‘„å–æ–‡æ¡£åˆ°çŸ¥è¯†åº“

        Args:
            file_path: æ–‡æ¡£æ–‡ä»¶è·¯å¾„
            chunk_size: æ–‡æœ¬åˆ†å‰²å—å¤§å°
            chunk_overlap: æ–‡æœ¬åˆ†å‰²é‡å å¤§å°

        Returns:
            å¤„ç†ç»“æœä¿¡æ¯
        """
        try:
            print(f"ğŸ“š å¼€å§‹æ‘„å–æ–‡æ¡£: {file_path}")

            # 1. åŠ è½½æ–‡æ¡£
            documents = self.loader.load_file(file_path)
            print(f"   åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")

            # 2. åˆ†å‰²æ–‡æ¡£
            self.splitter.chunk_size = chunk_size
            self.splitter.chunk_overlap = chunk_overlap
            split_documents = self.splitter.split_documents(documents)
            print(f"   åˆ†å‰²æˆ {len(split_documents)} ä¸ªæ–‡æœ¬å—")

            # 3. æ·»åŠ åˆ°å‘é‡å­˜å‚¨
            doc_ids = self.vector_store.add_documents(split_documents)

            # 4. è·å–é›†åˆä¿¡æ¯
            collection_info = self.vector_store.get_collection_info()

            result = {
                "success": True,
                "file_path": file_path,
                "original_documents": len(documents),
                "split_documents": len(split_documents),
                "added_documents": len(doc_ids),
                "collection_info": collection_info
            }

            print(f"âœ… æ–‡æ¡£æ‘„å–å®Œæˆ: æ·»åŠ äº† {len(doc_ids)} ä¸ªæ–‡æ¡£å—")
            return result

        except Exception as e:
            error_msg = f"æ–‡æ¡£æ‘„å–å¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            return {
                "success": False,
                "error": error_msg,
                "file_path": file_path
            }

    def search(self,
               question: str,
               k: int = 3,
               filters: Optional[Dict[str, Any]] = None) -> List[Document]:
        """
        ä»…æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼Œä¸ç”Ÿæˆç­”æ¡ˆ

        Args:
            question: æŸ¥è¯¢é—®é¢˜
            k: è¿”å›æ–‡æ¡£æ•°é‡
            filters: å…ƒæ•°æ®è¿‡æ»¤å™¨

        Returns:
            æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨
        """
        try:
            print(f"ğŸ” æ£€ç´¢é—®é¢˜: {question}")

            results = self.retriever.search(question, k=k, filters=filters)

            print(f"âœ… æ£€ç´¢å®Œæˆ: æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£")
            return results

        except Exception as e:
            raise Exception(f"æ£€ç´¢å¤±è´¥: {str(e)}")

    def query(self,
              question: str,
              k: int = 3,
              filters: Optional[Dict[str, Any]] = None,
              use_generator: Optional[bool] = None) -> Dict[str, Any]:
        """
        æŸ¥è¯¢çŸ¥è¯†åº“

        Args:
            question: æŸ¥è¯¢é—®é¢˜
            k: è¿”å›æ–‡æ¡£æ•°é‡
            filters: å…ƒæ•°æ®è¿‡æ»¤å™¨
            use_generator: æ˜¯å¦ä½¿ç”¨ç”Ÿæˆå™¨ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨ç³»ç»Ÿé»˜è®¤

        Returns:
            æŸ¥è¯¢ç»“æœ
        """
        try:
            print(f"â“ æŸ¥è¯¢é—®é¢˜: {question}")

            # ç¡®å®šæ˜¯å¦ä½¿ç”¨ç”Ÿæˆå™¨
            should_use_generator = use_generator if use_generator is not None else self.use_generator

            # æ£€ç´¢ç›¸å…³æ–‡æ¡£
            retrieved_docs = self.retriever.search(question, k=k, filters=filters)

            if not retrieved_docs:
                return {
                    "question": question,
                    "answer": "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„ä¿¡æ¯æ¥å›ç­”è¿™ä¸ªé—®é¢˜ã€‚",
                    "retrieved_documents": [],
                    "sources_count": 0,
                    "used_generator": False
                }

            # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä½¿ç”¨ç”Ÿæˆå™¨
            if should_use_generator and self.generator:
                print("ğŸ¤– ä½¿ç”¨ç”Ÿæˆå™¨ç”Ÿæˆç­”æ¡ˆ...")
                result = self.generator.generate_answer_with_sources(question, retrieved_docs)
                result["used_generator"] = True
            else:
                print("ğŸ” ç›´æ¥è¿”å›æ£€ç´¢ç»“æœ...")
                result = {
                    "question": question,
                    "answer": None,  # è¡¨ç¤ºæœªä½¿ç”¨ç”Ÿæˆå™¨
                    "retrieved_documents": retrieved_docs,
                    "sources_count": len(retrieved_docs),
                    "used_generator": False
                }

            print(f"âœ… æŸ¥è¯¢å®Œæˆ: æ‰¾åˆ° {len(retrieved_docs)} ä¸ªç›¸å…³æ–‡æ¡£")
            return result

        except Exception as e:
            raise Exception(f"æŸ¥è¯¢å¤±è´¥: {str(e)}")

    def get_system_info(self) -> Dict[str, Any]:
        """
        è·å–ç³»ç»Ÿä¿¡æ¯

        Returns:
            ç³»ç»Ÿä¿¡æ¯å­—å…¸
        """
        collection_info = self.vector_store.get_collection_info()

        return {
            "embedding_model": self.embedding_manager.get_model_info(),
            "collection_info": collection_info,
            "generator_enabled": self.use_generator,
            "generator_info": self.generator.get_model_info() if self.generator else None
        }

    def clear_knowledge_base(self) -> bool:
        """
        æ¸…ç©ºçŸ¥è¯†åº“

        Returns:
            æ˜¯å¦æˆåŠŸæ¸…ç©º
        """
        try:
            return self.vector_store.clear_collection()
        except Exception as e:
            raise Exception(f"æ¸…ç©ºçŸ¥è¯†åº“å¤±è´¥: {str(e)}")

    def ingest_directory(self,
                         directory_path: str,
                         file_extensions: List[str] = None,
                         chunk_size: int = 1000,
                         chunk_overlap: int = 200) -> Dict[str, Any]:
        """
        æ‰¹é‡æ‘„å–ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡æ¡£

        Args:
            directory_path: æ–‡æ¡£ç›®å½•è·¯å¾„
            file_extensions: æ”¯æŒçš„æ–‡ä»¶æ‰©å±•ååˆ—è¡¨ï¼Œé»˜è®¤æ”¯æŒå¸¸è§æ–‡æœ¬æ–‡ä»¶
            chunk_size: æ–‡æœ¬åˆ†å‰²å—å¤§å°
            chunk_overlap: æ–‡æœ¬åˆ†å‰²é‡å å¤§å°

        Returns:
            å¤„ç†ç»“æœä¿¡æ¯
        """
        try:
            import os

            print(f"ğŸ“š å¼€å§‹æ‰¹é‡æ‘„å–ç›®å½•: {directory_path}")

            # é»˜è®¤æ”¯æŒçš„æ–‡ä»¶æ‰©å±•å
            if file_extensions is None:
                file_extensions = ['.txt', '.md', '.pdf', '.docx', '.doc', '.html', '.htm']

            # æ”¶é›†æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶
            supported_files = []
            for root, dirs, files in os.walk(directory_path):
                for file in files:
                    file_ext = os.path.splitext(file)[1].lower()
                    if file_ext in file_extensions:
                        full_path = os.path.join(root, file)
                        supported_files.append(full_path)

            if not supported_files:
                return {
                    "success": False,
                    "error": f"åœ¨ç›®å½• {directory_path} ä¸­æœªæ‰¾åˆ°æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_extensions}",
                    "directory_path": directory_path
                }

            print(f"   æ‰¾åˆ° {len(supported_files)} ä¸ªæ”¯æŒçš„æ–‡ä»¶")

            total_documents = 0
            total_chunks = 0
            processed_files = []
            failed_files = []

            # é€ä¸ªå¤„ç†æ–‡ä»¶
            for file_path in supported_files:
                try:
                    print(f"   å¤„ç†æ–‡ä»¶: {os.path.basename(file_path)}")

                    # åŠ è½½æ–‡æ¡£
                    documents = self.loader.load_file(file_path)

                    # åˆ†å‰²æ–‡æ¡£
                    self.splitter.chunk_size = chunk_size
                    self.splitter.chunk_overlap = chunk_overlap
                    split_documents = self.splitter.split_documents(documents)

                    # æ·»åŠ åˆ°å‘é‡å­˜å‚¨
                    doc_ids = self.vector_store.add_documents(split_documents)

                    total_documents += len(documents)
                    total_chunks += len(split_documents)
                    processed_files.append({
                        "file_path": file_path,
                        "original_documents": len(documents),
                        "split_documents": len(split_documents),
                        "added_documents": len(doc_ids)
                    })

                    print(f"      âœ… æˆåŠŸæ·»åŠ  {len(split_documents)} ä¸ªæ–‡æ¡£å—")

                except Exception as e:
                    error_msg = f"å¤„ç†æ–‡ä»¶å¤±è´¥: {str(e)}"
                    print(f"      âŒ {error_msg}")
                    failed_files.append({
                        "file_path": file_path,
                        "error": error_msg
                    })

            # è·å–é›†åˆä¿¡æ¯
            collection_info = self.vector_store.get_collection_info()

            result = {
                "success": True,
                "directory_path": directory_path,
                "total_files": len(supported_files),
                "processed_files": len(processed_files),
                "failed_files": len(failed_files),
                "total_original_documents": total_documents,
                "total_split_documents": total_chunks,
                "processed_files_details": processed_files,
                "failed_files_details": failed_files,
                "collection_info": collection_info
            }

            print(f"âœ… æ‰¹é‡æ–‡æ¡£æ‘„å–å®Œæˆ: å¤„ç†äº† {len(processed_files)} ä¸ªæ–‡ä»¶ï¼Œæ·»åŠ äº† {total_chunks} ä¸ªæ–‡æ¡£å—")
            if failed_files:
                print(f"âš ï¸  æœ‰ {len(failed_files)} ä¸ªæ–‡ä»¶å¤„ç†å¤±è´¥")

            return result

        except Exception as e:
            error_msg = f"æ‰¹é‡æ–‡æ¡£æ‘„å–å¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            return {
                "success": False,
                "error": error_msg,
                "directory_path": directory_path
            }

    def get_knowledge_base_stats(self) -> Dict[str, Any]:
        """
        è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯

        Returns:
            çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            collection_info = self.vector_store.get_collection_info()

            return {
                "collection_info": collection_info,
                "system_info": self.get_system_info()
            }
        except Exception as e:
            raise Exception(f"è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")


# æ‰¹é‡æ–‡æ¡£æµ‹è¯•å‡½æ•°
def test_batch_ingest():
    """æµ‹è¯•æ‰¹é‡æ–‡æ¡£åŠ è½½åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•æ‰¹é‡æ–‡æ¡£åŠ è½½åŠŸèƒ½...")

    import tempfile
    import os
    import shutil

    # åˆ›å»ºæµ‹è¯•ç›®å½•å’Œæ–‡ä»¶
    test_dir = tempfile.mkdtemp()

    try:
        # åˆ›å»ºå¤šä¸ªæµ‹è¯•æ–‡ä»¶
        test_files = {
            "machine_learning.txt": """
æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ã€‚
å®ƒä¸“æ³¨äºå¼€å‘ç®—æ³•å’Œç»Ÿè®¡æ¨¡å‹ï¼Œä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ å¹¶åšå‡ºé¢„æµ‹æˆ–å†³ç­–ã€‚
""",
            "deep_learning.md": """
# æ·±åº¦å­¦ä¹ 

æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸã€‚
å®ƒä½¿ç”¨ç¥ç»ç½‘ç»œæ¥å¤„ç†å¤æ‚çš„æ¨¡å¼è¯†åˆ«ä»»åŠ¡ï¼Œå¦‚å›¾åƒè¯†åˆ«å’Œè‡ªç„¶è¯­è¨€å¤„ç†ã€‚
""",
            "nlp_doc.txt": """
è‡ªç„¶è¯­è¨€å¤„ç†(NLP)æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåº”ç”¨é¢†åŸŸã€‚
å®ƒè®©è®¡ç®—æœºèƒ½å¤Ÿç†è§£ã€è§£é‡Šå’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚
"""
        }

        # å†™å…¥æµ‹è¯•æ–‡ä»¶
        for filename, content in test_files.items():
            file_path = os.path.join(test_dir, filename)
            with open(file_path, "w", encoding="utf-8") as f:
                f.write(content.strip())

        print(f"ğŸ“ åˆ›å»ºæµ‹è¯•ç›®å½•: {test_dir}")
        print(f"   åŒ…å« {len(test_files)} ä¸ªæµ‹è¯•æ–‡ä»¶")

        # åˆå§‹åŒ–RAGç³»ç»Ÿ
        rag_system = RAGSystem(use_generator=False, persist_directory=test_dir + "_vector_store")

        # æ‰¹é‡æ‘„å–æ–‡æ¡£
        batch_result = rag_system.ingest_directory(test_dir)

        print(f"\nğŸ“Š æ‰¹é‡æ‘„å–ç»“æœ:")
        print(f"   æˆåŠŸå¤„ç†: {batch_result['processed_files']} ä¸ªæ–‡ä»¶")
        print(f"   å¤±è´¥æ–‡ä»¶: {batch_result['failed_files']} ä¸ª")
        print(f"   åŸå§‹æ–‡æ¡£: {batch_result['total_original_documents']} ä¸ª")
        print(f"   åˆ†å‰²å—æ•°: {batch_result['total_split_documents']} ä¸ª")

        # æµ‹è¯•æŸ¥è¯¢
        print(f"\nğŸ” æµ‹è¯•æŸ¥è¯¢åŠŸèƒ½...")

        # æŸ¥è¯¢æœºå™¨å­¦ä¹ ç›¸å…³
        ml_results = rag_system.search("ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ", k=2)
        print(f"   æœºå™¨å­¦ä¹ æŸ¥è¯¢: æ‰¾åˆ° {len(ml_results)} ä¸ªç›¸å…³æ–‡æ¡£")

        # æŸ¥è¯¢æ·±åº¦å­¦ä¹ ç›¸å…³
        dl_results = rag_system.search("æ·±åº¦å­¦ä¹ ", k=2)
        print(f"   æ·±åº¦å­¦ä¹ æŸ¥è¯¢: æ‰¾åˆ° {len(dl_results)} ä¸ªç›¸å…³æ–‡æ¡£")

        # è·å–çŸ¥è¯†åº“ç»Ÿè®¡
        stats = rag_system.get_knowledge_base_stats()
        print(f"\nğŸ“ˆ çŸ¥è¯†åº“ç»Ÿè®¡:")
        print(f"   å‘é‡å­˜å‚¨ç±»å‹: {stats['collection_info'].get('vector_store_type', 'N/A')}")
        print(f"   æ–‡æ¡£æ•°é‡: {stats['collection_info'].get('document_count', 0)}")

        print("âœ… æ‰¹é‡æ–‡æ¡£åŠ è½½æµ‹è¯•é€šè¿‡")

    finally:
        # æ¸…ç†æµ‹è¯•ç›®å½•
        try:
            shutil.rmtree(test_dir, ignore_errors=True)
        except:
            pass


# æµ‹è¯•å‡½æ•°
def test_rag_system():
    """æµ‹è¯•RAGç³»ç»Ÿ"""
    print("ğŸ§ª æµ‹è¯•RAGç³»ç»Ÿ...")

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    import tempfile
    import os

    # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
    test_content = """
æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ã€‚
å®ƒä¸“æ³¨äºå¼€å‘ç®—æ³•å’Œç»Ÿè®¡æ¨¡å‹ï¼Œä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ å¹¶åšå‡ºé¢„æµ‹æˆ–å†³ç­–ã€‚

æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸã€‚
å®ƒä½¿ç”¨ç¥ç»ç½‘ç»œæ¥å¤„ç†å¤æ‚çš„æ¨¡å¼è¯†åˆ«ä»»åŠ¡ï¼Œå¦‚å›¾åƒè¯†åˆ«å’Œè‡ªç„¶è¯­è¨€å¤„ç†ã€‚

è‡ªç„¶è¯­è¨€å¤„ç†(NLP)æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåº”ç”¨é¢†åŸŸã€‚
å®ƒè®©è®¡ç®—æœºèƒ½å¤Ÿç†è§£ã€è§£é‡Šå’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚
"""

    # ä½¿ç”¨æ‰‹åŠ¨åˆ›å»ºçš„ä¸´æ—¶ç›®å½•
    temp_dir = tempfile.mkdtemp()

    try:
        # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
        test_file = os.path.join(temp_dir, "test_ai.txt")
        with open(test_file, "w", encoding="utf-8") as f:
            f.write(test_content)

        # æµ‹è¯•1: ä¸ä½¿ç”¨ç”Ÿæˆå™¨
        print("\nğŸ” æµ‹è¯•ä¸ä½¿ç”¨ç”Ÿæˆå™¨æ¨¡å¼...")
        try:
            rag_system = RAGSystem(use_generator=False, persist_directory=temp_dir)

            # æ‘„å–æ–‡æ¡£
            ingest_result = rag_system.ingest_documents(test_file)
            print(f"   æ–‡æ¡£æ‘„å–: {ingest_result['success']}")

            # ä»…æ£€ç´¢
            search_results = rag_system.search("ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ", k=2)
            print(f"   ä»…æ£€ç´¢: æ‰¾åˆ° {len(search_results)} ä¸ªæ–‡æ¡£")

            # æŸ¥è¯¢ï¼ˆä¸ä½¿ç”¨ç”Ÿæˆå™¨ï¼‰
            query_result = rag_system.query("æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ çš„åŒºåˆ«ï¼Ÿ", k=2)
            print(f"   æŸ¥è¯¢ç»“æœ: ä½¿ç”¨ç”Ÿæˆå™¨={query_result['used_generator']}")
            print(f"   ç›¸å…³æ–‡æ¡£: {query_result['sources_count']} ä¸ª")

            # ç³»ç»Ÿä¿¡æ¯
            system_info = rag_system.get_system_info()
            print(f"   ç³»ç»Ÿä¿¡æ¯: ç”Ÿæˆå™¨å¯ç”¨={system_info['generator_enabled']}")

            print("âœ… ä¸ä½¿ç”¨ç”Ÿæˆå™¨æ¨¡å¼æµ‹è¯•é€šè¿‡")

        except Exception as e:
            print(f"   âš ï¸ ä¸ä½¿ç”¨ç”Ÿæˆå™¨æ¨¡å¼æµ‹è¯•å¤±è´¥: {str(e)}")

        # æµ‹è¯•2: ä½¿ç”¨ç”Ÿæˆå™¨ï¼ˆå¦‚æœOllamaå¯ç”¨ï¼‰
        print("\nğŸ¤– æµ‹è¯•ä½¿ç”¨ç”Ÿæˆå™¨æ¨¡å¼...")
        try:
            rag_system = RAGSystem(use_generator=True, persist_directory=temp_dir)

            # æŸ¥è¯¢ï¼ˆä½¿ç”¨ç”Ÿæˆå™¨ï¼‰
            query_result = rag_system.query("ä»€ä¹ˆæ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼Ÿ", k=2)
            print(f"   æŸ¥è¯¢ç»“æœ: ä½¿ç”¨ç”Ÿæˆå™¨={query_result['used_generator']}")

            if query_result['used_generator']:
                print(f"   ç”Ÿæˆç­”æ¡ˆ: {query_result['answer'][:100]}...")

            print("âœ… ä½¿ç”¨ç”Ÿæˆå™¨æ¨¡å¼æµ‹è¯•é€šè¿‡")

        except Exception as e:
            print(f"   âš ï¸ ä½¿ç”¨ç”Ÿæˆå™¨æ¨¡å¼æµ‹è¯•å¤±è´¥: {str(e)}")
            print("   å¯èƒ½æ˜¯OllamaæœåŠ¡æœªå¯åŠ¨")

    finally:
        # æ‰‹åŠ¨æ¸…ç†ï¼Œå¿½ç•¥é”™è¯¯
        try:
            shutil.rmtree(temp_dir, ignore_errors=True)
        except:
            pass

    print("\nğŸ¯ RAGç³»ç»Ÿæµ‹è¯•å®Œæˆ")

if __name__ == "__main__":
    # è¿è¡Œæ‰¹é‡æ–‡æ¡£æµ‹è¯•
    test_batch_ingest()
    print("\n" + "=" * 50)
    # è¿è¡ŒåŸæœ‰æµ‹è¯•
    test_rag_system()
