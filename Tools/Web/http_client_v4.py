"""
ğŸ± HTTPè·å–å·¥å…·

é›†æˆåŠŸèƒ½ï¼š
- HTTPè¯·æ±‚å’Œé‡è¯•æœºåˆ¶
- HTMLè§£æå’Œå†…å®¹æå–
- æ–‡æœ¬å¤„ç†å’Œä¼˜åŒ–
- æ™ºèƒ½å†…å®¹ç±»å‹æ£€æµ‹

ä½œè€…: Neko
ç‰ˆæœ¬: 4.0
"""

from langchain.tools import tool
import requests
import time
import re
from bs4 import BeautifulSoup
from typing import Dict, List, Optional, Union, Callable
from collections import Counter
import string
from urllib.parse import urljoin, urlparse


# ==================== è¾…åŠ©å‡½æ•° ====================

def _is_html_content(content_type: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºHTMLå†…å®¹"""
    return content_type and 'text/html' in content_type.lower()


def _build_error_response(error_msg: str, attempt: int, url: str, strategy: str) -> Dict:
    """æ„å»ºé”™è¯¯å“åº”"""
    return {
        'success': False,
        'error': error_msg,
        'status_code': 0,
        'url': url,
        'attempt': attempt,
        'optimization_strategy': strategy
    }


# ==================== HTMLè§£ææ¨¡å— ====================

def _parse_html_impl(
    content: str,
    base_url: str = "",
    extract_rules: Optional[Dict] = None,
    optimize: bool = True
) -> Dict[str, Union[str, List, Dict]]:
    """
    HTMLè§£æå®ç°å‡½æ•°

    Args:
        content: HTMLå†…å®¹
        base_url: åŸºç¡€URLï¼Œç”¨äºè§£æç›¸å¯¹é“¾æ¥
        extract_rules: æå–è§„åˆ™é…ç½®
            - title_selector: æ ‡é¢˜é€‰æ‹©å™¨
            - content_selector: å†…å®¹é€‰æ‹©å™¨
            - link_selector: é“¾æ¥é€‰æ‹©å™¨
            - remove_selectors: è¦ç§»é™¤çš„å…ƒç´ é€‰æ‹©å™¨åˆ—è¡¨
        optimize: æ˜¯å¦ä¼˜åŒ–å†…å®¹ï¼ˆæ¸…ç†å™ªéŸ³ï¼‰

    Returns:
        Dict: åŒ…å«è§£æç»“æœçš„å­—å…¸
    """

    # é»˜è®¤æå–è§„åˆ™
    default_rules = {
        'title_selector': 'title, h1, .title, .heading',
        'content_selector': 'main, article, .content, .main-content, body',
        'link_selector': 'a[href]',
        'remove_selectors': [
            'script', 'style', 'nav', 'header', 'footer',
            '.ad', '.ads', '.advertisement', '.navigation',
            '.menu', '.sidebar', '.footer', '.nextra-toc'
        ]
    }

    # åˆå¹¶è§„åˆ™
    if extract_rules:
        default_rules.update(extract_rules)

    rules = default_rules

    try:
        # åˆ›å»ºBeautifulSoupå¯¹è±¡
        soup = BeautifulSoup(content, 'html.parser')

        # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
        for selector in rules['remove_selectors']:
            for element in soup.select(selector):
                element.decompose()

        # æå–æ ‡é¢˜
        title = ""
        for selector in rules['title_selector'].split(', '):
            title_elem = soup.select_one(selector.strip())
            if title_elem and title_elem.get_text().strip():
                title = title_elem.get_text().strip()
                break

        # æå–ä¸»è¦å†…å®¹
        content_text = ""
        content_elem = soup.select_one(rules['content_selector'])
        if content_elem:
            content_text = content_elem.get_text().strip()
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šå†…å®¹åŒºåŸŸï¼Œä½¿ç”¨æ•´ä¸ªbody
            body_elem = soup.find('body')
            if body_elem:
                content_text = body_elem.get_text().strip()

        # ä¼˜åŒ–å†…å®¹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if optimize:
            content_text = _clean_text(content_text)

        # æå–é“¾æ¥
        links = _extract_links(soup, base_url, rules['link_selector'])

        # æå–å…ƒæ•°æ®
        metadata = _extract_metadata(soup)

        # æå–ä»£ç å—ï¼ˆå¯¹äºæ–‡æ¡£é¡µé¢å¾ˆé‡è¦ï¼‰
        code_blocks = _extract_code_blocks(soup)

        return {
            'success': True,
            'title': title,
            'content': content_text,
            'content_length': len(content_text),
            'links': links,
            'metadata': metadata,
            'code_blocks': code_blocks,
            'link_count': len(links),
            'code_block_count': len(code_blocks),
            'optimized': optimize
        }

    except Exception as e:
        return {
            'success': False,
            'error': f'HTMLè§£æé”™è¯¯: {str(e)}',
            'title': '',
            'content': '',
            'links': [],
            'metadata': {},
            'code_blocks': [],
            'optimized': optimize
        }


def _extract_links(soup: BeautifulSoup, base_url: str, selector: str) -> List[Dict]:
    """æå–é¡µé¢ä¸­çš„æ‰€æœ‰é“¾æ¥"""
    links = []
    seen_urls = set()

    for link_elem in soup.select(selector):
        href = link_elem.get('href', '').strip()
        if not href or href.startswith('javascript:') or href == '#':
            continue

        # è§£æå®Œæ•´URL
        full_url = urljoin(base_url, href) if base_url else href

        # å»é‡
        if full_url in seen_urls:
            continue
        seen_urls.add(full_url)

        # æå–é“¾æ¥ä¿¡æ¯
        link_info = {
            'url': full_url,
            'text': link_elem.get_text().strip(),
            'title': link_elem.get('title', ''),
            'is_external': _is_external_link(full_url, base_url)
        }

        links.append(link_info)

    return links


def _extract_metadata(soup: BeautifulSoup) -> Dict[str, str]:
    """æå–é¡µé¢å…ƒæ•°æ®"""
    metadata = {}

    # æå–metaæ ‡ç­¾
    for meta in soup.find_all('meta'):
        name = meta.get('name') or meta.get('property')
        content = meta.get('content', '')

        if name and content:
            metadata[name] = content

    return metadata


def _extract_code_blocks(soup: BeautifulSoup, max_blocks: int = 10) -> List[Dict]:
    """æå–ä»£ç å—"""
    code_blocks = []

    for code_elem in soup.find_all(['code', 'pre']):
        code_text = code_elem.get_text().strip()
        if len(code_text) > 10:  # åªä¿ç•™æœ‰æ„ä¹‰çš„ä»£ç å—
            code_blocks.append({
                'content': code_text,
                'language': _detect_code_language(code_elem),
                'length': len(code_text)
            })
            if len(code_blocks) >= max_blocks:
                break

    return code_blocks


def _detect_code_language(code_elem) -> str:
    """æ£€æµ‹ä»£ç è¯­è¨€"""
    # ç®€å•çš„è¯­è¨€æ£€æµ‹
    class_attr = code_elem.get('class', [])
    if class_attr:
        for cls in class_attr:
            if 'language-' in cls:
                return cls.replace('language-', '')

    # æ ¹æ®å†…å®¹æ¨æµ‹
    code_text = code_elem.get_text()
    if 'def ' in code_text or 'import ' in code_text:
        return 'python'
    elif 'function' in code_text or 'const ' in code_text:
        return 'javascript'
    elif '<' in code_text and '>' in code_text:
        return 'html'
    else:
        return 'unknown'


def _clean_text(text: str) -> str:
    """æ¸…ç†æ–‡æœ¬å†…å®¹"""
    if not text:
        return ""

    # ç§»é™¤å¤šä½™çš„ç©ºæ ¼å’Œæ¢è¡Œ
    text = re.sub(r'\s+', ' ', text)

    # ç§»é™¤é¦–å°¾ç©ºæ ¼
    text = text.strip()

    return text


def _is_external_link(url: str, base_url: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºå¤–éƒ¨é“¾æ¥"""
    if not base_url:
        return False

    try:
        base_domain = urlparse(base_url).netloc
        url_domain = urlparse(url).netloc

        return bool(url_domain and url_domain != base_domain)
    except:
        return False


# ==================== æ–‡æœ¬å¤„ç†æ¨¡å— ====================

def _process_text_impl(
    content: str,
    filters: Optional[List[str]] = None,
    max_length: Optional[int] = None,
    generate_summary: bool = False,
    extract_keywords: bool = False,
    keyword_count: int = 10,
    optimize_strategy: str = "smart"
) -> Dict[str, Union[str, List, Dict]]:
    """
    æ–‡æœ¬å¤„ç†å®ç°å‡½æ•°ï¼ˆç®€æ´ç‰ˆï¼‰

    Args:
        content: åŸå§‹æ–‡æœ¬å†…å®¹
        filters: è¿‡æ»¤å™¨åˆ—è¡¨
            - 'remove_extra_spaces': ç§»é™¤å¤šä½™ç©ºæ ¼
            - 'remove_special_chars': ç§»é™¤ç‰¹æ®Šå­—ç¬¦
            - 'normalize_newlines': æ ‡å‡†åŒ–æ¢è¡Œç¬¦
            - 'remove_numbers': ç§»é™¤æ•°å­—
        max_length: æœ€å¤§æ–‡æœ¬é•¿åº¦ï¼ˆæˆªæ–­ï¼‰
        generate_summary: æ˜¯å¦ç”Ÿæˆæ‘˜è¦
        extract_keywords: æ˜¯å¦æå–å…³é”®è¯
        keyword_count: å…³é”®è¯æ•°é‡
        optimize_strategy: ä¼˜åŒ–ç­–ç•¥
            - "smart": æ™ºèƒ½ä¼˜åŒ–
            - "chunk": åˆ†å—å¤„ç†
            - "summary": æ‘˜è¦ç”Ÿæˆ

    Returns:
        Dict: åŒ…å«å¤„ç†ç»“æœçš„å­—å…¸
    """

    # ç©ºå†…å®¹ç›´æ¥è¿”å›
    if not content:
        return {
            'success': True,
            'processed_text': '',
            'original_length': 0,
            'processed_length': 0,
            'reduction_ratio': 0,
            'stats': {'char_count': 0, 'word_count': 0, 'sentence_count': 0, 'paragraph_count': 1},
            'optimization_strategy': optimize_strategy
        }

    # é»˜è®¤è¿‡æ»¤å™¨
    default_filters = [
        'remove_extra_spaces',
        'normalize_newlines',
        'remove_special_chars'
    ]

    if filters:
        active_filters = filters
    else:
        active_filters = default_filters

    try:
        # åº”ç”¨è¿‡æ»¤å™¨
        processed_text = content

        for filter_name in active_filters:
            if filter_name == 'remove_extra_spaces':
                processed_text = _remove_extra_spaces(processed_text)
            elif filter_name == 'remove_special_chars':
                processed_text = _remove_special_chars(processed_text)
            elif filter_name == 'normalize_newlines':
                processed_text = _normalize_newlines(processed_text)
            elif filter_name == 'remove_numbers':
                processed_text = _remove_numbers(processed_text)

        # æ ¹æ®ä¼˜åŒ–ç­–ç•¥è¿›ä¸€æ­¥å¤„ç†
        if optimize_strategy == "summary" and generate_summary:
            # æ‘˜è¦ç­–ç•¥
            summary = _generate_text_summary(processed_text, summary_ratio=0.3)
            processed_text = summary
        elif optimize_strategy == "chunk" and max_length:
            # åˆ†å—å¤„ç†
            processed_text = _chunk_content(processed_text, max_length)
        elif optimize_strategy == "smart" and max_length and len(processed_text) > max_length:
            # æ™ºèƒ½ä¼˜åŒ–
            processed_text = _smart_optimize_content(processed_text, max_length)

        # é•¿åº¦é™åˆ¶
        original_length = len(processed_text)
        if max_length and len(processed_text) > max_length:
            processed_text = processed_text[:max_length] + "..."

        # ç»Ÿè®¡ä¿¡æ¯
        stats = _calculate_text_stats(processed_text)

        result = {
            'success': True,
            'processed_text': processed_text,
            'original_length': len(content),
            'processed_length': len(processed_text),
            'reduction_ratio': (len(content) - len(processed_text)) / len(content),
            'stats': stats,
            'optimization_strategy': optimize_strategy
        }

        # ç”Ÿæˆæ‘˜è¦ï¼ˆå¦‚æœæœªåœ¨ä¼˜åŒ–ä¸­ç”Ÿæˆï¼‰
        if generate_summary and optimize_strategy != "summary":
            summary = _generate_text_summary(processed_text)
            result['summary'] = summary

        # æå–å…³é”®è¯
        if extract_keywords:
            keywords = _extract_text_keywords(processed_text, keyword_count)
            result['keywords'] = keywords

        return result

    except Exception as e:
        return {
            'success': False,
            'error': f'æ–‡æœ¬å¤„ç†é”™è¯¯: {str(e)}',
            'processed_text': '',
            'original_length': 0,
            'processed_length': 0,
            'stats': {},
            'optimization_strategy': optimize_strategy
        }


def _remove_extra_spaces(text: str) -> str:
    """ç§»é™¤å¤šä½™ç©ºæ ¼"""
    text = re.sub(r'\s+', ' ', text)
    return text.strip()


def _remove_special_chars(text: str) -> str:
    """ç§»é™¤ç‰¹æ®Šå­—ç¬¦"""
    pattern = r'[^\w\s\u4e00-\u9fff.,!?;:()\-\'\"]'
    return re.sub(pattern, '', text)


def _normalize_newlines(text: str) -> str:
    """æ ‡å‡†åŒ–æ¢è¡Œç¬¦"""
    text = re.sub(r'\n\s*\n', '\n\n', text)
    return text


def _remove_numbers(text: str) -> str:
    """ç§»é™¤æ•°å­—"""
    return re.sub(r'\d+', '', text)


def _smart_optimize_content(text: str, max_length: int) -> str:
    """æ™ºèƒ½ä¼˜åŒ–å†…å®¹"""
    sentences = re.split(r'[.!?]+', text)
    sentences = [s.strip() for s in sentences if s.strip()]

    # ç»™å¥å­æ‰“åˆ†
    scored_sentences = []
    for sentence in sentences:
        score = _score_sentence_importance(sentence)
        scored_sentences.append((sentence, score))

    # æŒ‰é‡è¦æ€§æ’åº
    scored_sentences.sort(key=lambda x: x[1], reverse=True)

    # é€‰æ‹©æœ€é‡è¦çš„å¥å­
    result = ""
    for sentence, score in scored_sentences:
        if len(result) + len(sentence) + 2 <= max_length:
            result += sentence + ". "
        else:
            break

    return result.strip() if result else text[:max_length]


def _score_sentence_importance(sentence: str) -> float:
    """ç»™å¥å­é‡è¦æ€§æ‰“åˆ†"""
    score = 0.0
    sentence_lower = sentence.lower()

    # å…³é”®è¯åŠ åˆ†
    important_keywords = [
        'é‡è¦', 'å…³é”®', 'æ³¨æ„', 'è­¦å‘Š', 'ç¤ºä¾‹', 'ä»£ç ',
        'important', 'key', 'note', 'warning', 'example', 'code'
    ]

    for keyword in important_keywords:
        if keyword in sentence_lower:
            score += 2.0

    # é•¿åº¦é€‚ä¸­åŠ åˆ†
    sentence_length = len(sentence)
    if 20 <= sentence_length <= 200:
        score += 1.0

    return score


def _chunk_content(text: str, max_chunk_size: int) -> str:
    """åˆ†å—å¤„ç†å†…å®¹"""
    paragraphs = re.split(r'\n\s*\n', text)

    chunks = []
    current_chunk = ""

    for paragraph in paragraphs:
        if len(current_chunk) + len(paragraph) + 2 <= max_chunk_size:
            current_chunk += paragraph + "\n\n"
        else:
            if current_chunk:
                chunks.append(current_chunk.strip())
            current_chunk = paragraph + "\n\n"

    if current_chunk:
        chunks.append(current_chunk.strip())

    return chunks[0] if chunks else ""


def _calculate_text_stats(text: str) -> Dict[str, int]:
    """è®¡ç®—æ–‡æœ¬ç»Ÿè®¡ä¿¡æ¯"""
    words = text.split()
    sentences = re.split(r'[.!?]+', text)
    sentences = [s.strip() for s in sentences if s.strip()]

    word_count = len(words)
    sentence_count = len(sentences)

    return {
        'char_count': len(text),
        'word_count': word_count,
        'sentence_count': sentence_count,
        'paragraph_count': text.count('\n\n') + 1,
        'avg_word_length': sum(len(word) for word in words) / word_count if word_count > 0 else 0,
        'avg_sentence_length': word_count / sentence_count if sentence_count > 0 else 0
    }


def _generate_text_summary(text: str, summary_ratio: float = 0.3) -> str:
    """ç”Ÿæˆæ–‡æœ¬æ‘˜è¦"""
    if not text:
        return ""

    # æŒ‰å¥å­åˆ†å‰²
    sentences = re.split(r'[.!?]+', text)
    sentences = [s.strip() for s in sentences if s.strip()]

    # ç»™å¥å­æ‰“åˆ†
    scored_sentences = []
    for sentence in sentences:
        score = _score_sentence_importance(sentence)
        scored_sentences.append((sentence, score))

    # æŒ‰é‡è¦æ€§æ’åº
    scored_sentences.sort(key=lambda x: x[1], reverse=True)

    # é€‰æ‹©æœ€é‡è¦çš„å¥å­
    summary_length = max(1, int(len(sentences) * summary_ratio))
    summary_sentences = [s[0] for s in scored_sentences[:summary_length]]

    # æŒ‰åŸæ–‡é¡ºåºæ’åº
    summary_sentences_sorted = []
    for sentence in sentences:
        if sentence in summary_sentences:
            summary_sentences_sorted.append(sentence)

    return '. '.join(summary_sentences_sorted) + '.'


def _extract_text_keywords(text: str, count: int = 10) -> List[Dict[str, Union[str, int]]]:
    """æå–æ–‡æœ¬å…³é”®è¯"""
    if not text:
        return []

    # åˆ†è¯
    words = text.lower().split()

    # ç§»é™¤åœç”¨è¯å’ŒçŸ­è¯
    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
    filtered_words = [
        word.strip(string.punctuation)
        for word in words
        if len(word) > 2 and word not in stop_words
    ]

    # è®¡ç®—è¯é¢‘
    word_freq = Counter(filtered_words)

    # è¿”å›å‰Nä¸ªå…³é”®è¯
    keywords = []
    for word, freq in word_freq.most_common(count):
        keywords.append({
            'word': word,
            'frequency': freq,
            'score': freq / len(filtered_words) if filtered_words else 0
        })

    return keywords

# ==================== å¤„ç†ç­–ç•¥å±‚ ====================

def _handle_raw_mode(content: str, response, max_content_length: Optional[int]) -> Dict:
    """å¤„ç†rawæ¨¡å¼"""
    content_length = len(content)

    if max_content_length and content_length > max_content_length:
        truncated_content = content[:max_content_length] + "..."
        return {
            'success': True,
            'status_code': response.status_code,
            'content': truncated_content,
            'headers': dict(response.headers),
            'url': response.url,
            'encoding': response.encoding,
            'content_type': response.headers.get('content-type', ''),
            'content_length': content_length,
            'content_optimized': False,
            'optimization_strategy': 'raw',
            'content_truncated': True
        }
    else:
        return {
            'success': True,
            'status_code': response.status_code,
            'content': content,
            'headers': dict(response.headers),
            'url': response.url,
            'encoding': response.encoding,
            'content_type': response.headers.get('content-type', ''),
            'content_length': content_length,
            'content_optimized': False,
            'optimization_strategy': 'raw',
            'content_truncated': False
        }


def _handle_parse_mode(content: str, response, max_content_length: Optional[int], url: str) -> Dict:
    """å¤„ç†parseæ¨¡å¼ - ä½¿ç”¨é›†æˆçš„HTMLè§£æ"""
    content_type = response.headers.get('content-type', '')

    if _is_html_content(content_type):
        # HTMLå†…å®¹ï¼Œä½¿ç”¨é›†æˆçš„HTMLè§£æ
        parse_result = _parse_html_impl(content, base_url=url)

        if parse_result['success']:
            parsed_content = parse_result['content']

            # é•¿åº¦é™åˆ¶
            if max_content_length and len(parsed_content) > max_content_length:
                parsed_content = parsed_content[:max_content_length] + "..."

            return {
                'success': True,
                'status_code': response.status_code,
                'content': parsed_content,
                'headers': dict(response.headers),
                'url': response.url,
                'encoding': response.encoding,
                'content_type': content_type,
                'content_length': len(content),
                'content_optimized': True,
                'optimization_strategy': 'parse',
                'content_truncated': len(parsed_content) < len(content)
            }
        else:
            # HTMLè§£æå¤±è´¥ï¼Œè¿”å›åŸå§‹å†…å®¹
            return _handle_raw_mode(content, response, max_content_length)
    else:
        # éHTMLå†…å®¹ï¼Œç›´æ¥è¿”å›åŸå§‹
        return _handle_raw_mode(content, response, max_content_length)


def _handle_smart_mode(content: str, response, max_content_length: Optional[int], url: str) -> Dict:
    """å¤„ç†smartæ¨¡å¼ - ä½¿ç”¨é›†æˆçš„HTMLè§£æå’Œæ–‡æœ¬å¤„ç†"""
    content_type = response.headers.get('content-type', '')

    if _is_html_content(content_type):
        # HTMLå†…å®¹ï¼Œè¿›è¡Œå®Œæ•´ä¼˜åŒ–æµç¨‹
        parse_result = _parse_html_impl(content, base_url=url)

        if parse_result['success']:
            # ä½¿ç”¨é›†æˆçš„æ–‡æœ¬å¤„ç†
            process_result = _process_text_impl(
                parse_result['content'],
                max_length=max_content_length,
                generate_summary=False,
                optimize_strategy="smart"
            )

            if process_result['success']:
                return {
                    'success': True,
                    'status_code': response.status_code,
                    'content': process_result['processed_text'],
                    'headers': dict(response.headers),
                    'url': response.url,
                    'encoding': response.encoding,
                    'content_type': content_type,
                    'content_length': len(content),
                    'content_optimized': True,
                    'optimization_strategy': 'smart',
                    'content_truncated': process_result['processed_length'] < len(parse_result['content'])
                }
            else:
                # æ–‡æœ¬å¤„ç†å¤±è´¥ï¼Œè¿”å›è§£æåçš„å†…å®¹
                return _handle_parse_mode(content, response, max_content_length, url)
        else:
            # HTMLè§£æå¤±è´¥ï¼Œè¿”å›åŸå§‹å†…å®¹
            return _handle_raw_mode(content, response, max_content_length)
    else:
        # éHTMLå†…å®¹ï¼Œç›´æ¥è¿”å›åŸå§‹
        return _handle_raw_mode(content, response, max_content_length)

# ==================== HTTPè¯·æ±‚æ ¸å¿ƒå®ç° ====================

def _get_http_impl(
    url: str,
    method: str = "GET",
    headers: Optional[Dict] = None,
    data: Optional[Dict] = None,
    timeout: int = 30,
    max_retries: int = 3,
    retry_delay: float = 1.0,
    max_content_length: Optional[int] = 15000,
    optimize_strategy: str = "raw",
    encoding: Optional[str] = None  # ğŸ†• æ–°å¢ç¼–ç å‚æ•°
) -> Dict[str, Union[str, int, bool, Dict]]:
    """
    HTTPè·å–å·¥å…·ï¼ˆå®Œå…¨é›†æˆç‰ˆï¼‰

    Args:
        url: è¯·æ±‚çš„URLåœ°å€
        method: è¯·æ±‚æ–¹æ³•ï¼ŒGETæˆ–POST
        headers: è‡ªå®šä¹‰è¯·æ±‚å¤´
        data: POSTè¯·æ±‚çš„æ•°æ®
        timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        retry_delay: é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
        max_content_length: æœ€å¤§å†…å®¹é•¿åº¦
        optimize_strategy: å†…å®¹ä¼˜åŒ–ç­–ç•¥
            - "raw": è¿”å›åŸå§‹å†…å®¹
            - "parse": HTMLè§£æä¸ºçº¯æ–‡æœ¬ï¼Œå…¶ä»–è¿”å›åŸå§‹
            - "smart": HTMLä¼˜åŒ–å¤„ç†ï¼Œå…¶ä»–è¿”å›åŸå§‹
        encoding: æ‰‹åŠ¨æŒ‡å®šç¼–ç ï¼ˆå¯é€‰ï¼‰
            - ç”¨äºè§£å†³ä¸­æ–‡ç½‘ç«™ä¹±ç é—®é¢˜
            - ç¤ºä¾‹ï¼š"GBK"ï¼ˆäººæ°‘ç½‘ã€å¤®è§†ç½‘ç­‰ï¼‰
            - ç¤ºä¾‹ï¼š"UTF-8"ï¼ˆç°ä»£ç½‘ç«™ï¼‰
            - ä¸æŒ‡å®šæ—¶ä½¿ç”¨è‡ªåŠ¨æ£€æµ‹

    Returns:
        Dict: åŒ…å«å“åº”çŠ¶æ€ã€å†…å®¹ã€å¤´ä¿¡æ¯ç­‰çš„å­—å…¸
    """

    # éªŒè¯ç­–ç•¥å‚æ•°
    valid_strategies = ["raw", "parse", "smart"]
    if optimize_strategy not in valid_strategies:
        return _build_error_response(
            f'æ— æ•ˆçš„ä¼˜åŒ–ç­–ç•¥: {optimize_strategy}ï¼Œå¯ç”¨ç­–ç•¥: {valid_strategies}',
            0, url, optimize_strategy
        )

    # é»˜è®¤è¯·æ±‚å¤´
    default_headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Neko-Crawler/4.0'
    }

    # åˆå¹¶è¯·æ±‚å¤´
    if headers:
        default_headers.update(headers)

    # é‡è¯•æœºåˆ¶
    for attempt in range(max_retries):
        try:
            # å‘é€è¯·æ±‚
            if method.upper() == "GET":
                response = requests.get(
                    url,
                    headers=default_headers,
                    timeout=timeout,
                    allow_redirects=True
                )
            elif method.upper() == "POST":
                response = requests.post(
                    url,
                    headers=default_headers,
                    data=data,
                    timeout=timeout,
                    allow_redirects=True
                )
            else:
                return _build_error_response(
                    f'ä¸æ”¯æŒçš„è¯·æ±‚æ–¹æ³•: {method}',
                    attempt + 1, url, optimize_strategy
                )

            # æ£€æŸ¥å“åº”çŠ¶æ€
            if response.status_code == 200:
                if encoding:
                    response.encoding = encoding
                content = response.text

                # æ ¹æ®ç­–ç•¥åˆ†å‘å¤„ç†
                if optimize_strategy == "raw":
                    return _handle_raw_mode(content, response, max_content_length)
                elif optimize_strategy == "parse":
                    return _handle_parse_mode(content, response, max_content_length, url)
                elif optimize_strategy == "smart":
                    return _handle_smart_mode(content, response, max_content_length, url)

            else:
                # é200çŠ¶æ€ç 
                return {
                    'success': True,  # è¯·æ±‚æˆåŠŸï¼Œåªæ˜¯æœåŠ¡å™¨è¿”å›é”™è¯¯
                    'status_code': response.status_code,
                    'content': response.text,
                    'headers': dict(response.headers),
                    'url': response.url,
                    'encoding': response.encoding,
                    'error': f'HTTPçŠ¶æ€ç : {response.status_code}',
                    'attempt': attempt + 1,
                    'content_optimized': False,
                    'optimization_strategy': optimize_strategy
                }

        except requests.exceptions.Timeout:
            error_msg = f"è¯·æ±‚è¶…æ—¶ (å°è¯• {attempt + 1}/{max_retries})"
        except requests.exceptions.ConnectionError:
            error_msg = f"è¿æ¥é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries})"
        except requests.exceptions.HTTPError as e:
            error_msg = f"HTTPé”™è¯¯: {e} (å°è¯• {attempt + 1}/{max_retries})"
        except requests.exceptions.RequestException as e:
            error_msg = f"è¯·æ±‚å¼‚å¸¸: {e} (å°è¯• {attempt + 1}/{max_retries})"
        except Exception as e:
            error_msg = f"æœªçŸ¥é”™è¯¯: {e} (å°è¯• {attempt + 1}/{max_retries})"

        # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç­‰å¾…åé‡è¯•
        if attempt < max_retries - 1:
            time.sleep(retry_delay)

    # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
    return _build_error_response(error_msg, max_retries, url, optimize_strategy)

# ==================== ä¸»å·¥å…·æ¥å£ ====================

@tool
def get_http(
    url: str,
    method: str = "GET",
    headers: Optional[Dict] = None,
    data: Optional[Dict] = None,
    timeout: int = 30,
    max_retries: int = 3,
    retry_delay: float = 1.0,
    max_content_length: Optional[int] = 15000,
    optimize_strategy: str = "raw",
    encoding: Optional[str] = None  # ğŸ†• æ–°å¢ç¼–ç å‚æ•°
) -> Dict[str, Union[str, int, bool, Dict]]:
    """
    HTTPè¯·æ±‚å®ç°å‡½æ•°ï¼ˆå®Œå…¨é›†æˆç‰ˆï¼‰

    Args:
        url: è¯·æ±‚çš„URLåœ°å€
        method: è¯·æ±‚æ–¹æ³•ï¼ŒGETæˆ–POST
        headers: è‡ªå®šä¹‰è¯·æ±‚å¤´
        data: POSTè¯·æ±‚çš„æ•°æ®
        timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        retry_delay: é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
        max_content_length: æœ€å¤§å†…å®¹é•¿åº¦
        optimize_strategy: å†…å®¹ä¼˜åŒ–ç­–ç•¥
            - "raw": è¿”å›åŸå§‹å†…å®¹
            - "parse": HTMLè§£æä¸ºçº¯æ–‡æœ¬ï¼Œå…¶ä»–è¿”å›åŸå§‹
            - "smart": HTMLä¼˜åŒ–å¤„ç†ï¼Œå…¶ä»–è¿”å›åŸå§‹
        encoding: æ‰‹åŠ¨æŒ‡å®šç¼–ç ï¼ˆå¯é€‰ï¼‰
            - ç”¨äºè§£å†³ä¸­æ–‡ç½‘ç«™ä¹±ç é—®é¢˜
            - ç¤ºä¾‹ï¼š"GBK"
            - ç¤ºä¾‹ï¼š"UTF-8"ï¼ˆäººæ°‘ç½‘ã€ä¸­æ–‡ç½‘ç«™ï¼‰
            - ä¸æŒ‡å®šæ—¶ä½¿ç”¨è‡ªåŠ¨æ£€æµ‹

    Returns:
        Dict: åŒ…å«å“åº”çŠ¶æ€ã€å†…å®¹ã€å¤´ä¿¡æ¯ç­‰çš„å­—å…¸
    """
    return _get_http_impl(
        url=url,
        method=method,
        headers=headers,
        data=data,
        timeout=timeout,
        max_retries=max_retries,
        retry_delay=retry_delay,
        max_content_length=max_content_length,
        optimize_strategy=optimize_strategy,
        encoding=encoding  # ğŸ†• æ–°å¢å‚æ•°ä¼ é€’
    )


# ==================== æµ‹è¯•å‡½æ•° ====================

def test_http_client_integrated():
    """æµ‹è¯•å®Œå…¨é›†æˆç‰ˆHTTPå®¢æˆ·ç«¯åŠŸèƒ½"""
    print("ğŸ± æµ‹è¯•å®Œå…¨é›†æˆç‰ˆHTTPè·å–å·¥å…·...")

    test_urls = [
        ("http://httpbin.org/get", "JSONé¡µé¢"),
        ("http://httpbin.org/html", "HTMLé¡µé¢"),
        ("http://example.com", "æ ‡å‡†HTMLé¡µé¢")
    ]

    for url, description in test_urls:
        print(f"\nğŸ¯ æµ‹è¯•URL: {description}")
        print("-" * 40)

        for strategy in ["raw", "parse", "smart"]:
            print(f"ğŸ“‹ æµ‹è¯•æ¨¡å¼: {strategy}")
            result = _get_http_impl(
                url,
                max_content_length=5000,
                optimize_strategy=strategy
            )

            print(f"è¯·æ±‚æˆåŠŸ: {result['success']}")
            print(f"çŠ¶æ€ç : {result.get('status_code', 'N/A')}")
            print(f"ä¼˜åŒ–ç­–ç•¥: {result.get('optimization_strategy', 'N/A')}")
            print(f"å†…å®¹ä¼˜åŒ–: {result.get('content_optimized', False)}")

            if result['success']:
                content_preview = result['content'][:100] if result['content'] else "[ç©ºå†…å®¹]"
                print(f"å†…å®¹é¢„è§ˆ: {content_preview}...")
                print("âœ… æ¨¡å¼æ­£å¸¸å·¥ä½œ")
            else:
                print(f"âŒ é”™è¯¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

    return True


if __name__ == "__main__":
    success = test_http_client_integrated()
    if success:
        print("\nâœ… å®Œå…¨é›†æˆç‰ˆHTTPå®¢æˆ·ç«¯æµ‹è¯•é€šè¿‡ï¼")
    else:
        print("\nâŒ å®Œå…¨é›†æˆç‰ˆHTTPå®¢æˆ·ç«¯æµ‹è¯•å¤±è´¥ï¼")
